\subsection{Environments of cooperative MARL}
In the field of reinforcement learning, the design and study of cooperative environments are crucial for the development of multi-agent systems. Cooperative reinforcement learning environments typically involve multiple agents working together to achieve a common goal, requiring collaboration among agents. Some popular environments such as Multi-Agent Particle Environment (MPE)\cite{lowe2017multi}, StarCraft II \cite{vinyals2017starcraft}, GRF\cite{kurach2020google} and Hanabi\cite{bard2020hanabi} are widely used and typically feature single and well-defined objectives, which remain unchanged despite the randomness of state transitions. 

In contrast, our proposed ComplexOvercooked is a multi-objective simulation environment where task objectives (orders) are dynamically updated. This introduces additional non-stationarity into the learning process of MARL agents. Furthermore, Overcooked is inherently a communication-based game, where players collaborate through natural language during gameplay. ComplexOvercooked fully considers this aspect and, in addition to human and RL agent controls, provides an interface for LLM control. This facilitates future research on LLM-enhanced reinforcement learning.
% 现有的合作任务仿真环境的任务目标单一且明确，且不会因为环境状态转移的随机性而改变。我们提出的ComplexOvercooled是一个多目标的仿真环境，任务目标（订单）会随机更新，这为MARL智能体的学习引入了更多非平稳性，此外，Overcooked本身是一款交流游戏，在合作的过程中，玩家之间通过自然语言进行交流，ComplexOvercooled充分考虑了这一事实，除了人和RL智能体，开发了大模型控制的接口，这有助于大模型增强的强化学习未来研究。
\subsection{Cooperative multi-agent reinforcement learning}
Although cooperative Multi-Agent Reinforcement Learning (MARL) has emerged as a powerful framework for addressing large-scale, complex, real-time, and uncertain problems in cooperative scenarios, it also introduces several challenges that need to be addressed. First, in partially observable environments, agents can only access local information, making it difficult for them to make optimal decisions independently\cite{zhu2022survey}. Second, the simultaneous learning of multiple agents results in a non-stationary environment, complicating policy convergence \cite{papoudakis2019dealing}. Third, cooperative MAS often relies on shared rewards, raising the challenge of credit assignment—how to allocate rewards effectively to guide individual agents toward cooperative behavior and maximize system performance \cite{wang2021towards}. Finally, as the number of agents increases, the search space for policy learning grows exponentially, posing scalability issues and making policy optimization increasingly difficult \cite{zhang2011scaling,christianos2021scaling}.

To tackle these challenges, researchers have developed a variety of algorithms to enhance agent cooperation, including policy gradient-based methods like MADDPG \cite{lowe2017multi} and MAPPO \cite{yu2022surprising}, value-based methods such as VDN \cite{sunehag2017value} and QMIX \cite{rashid2020monotonic}, and Transformer-based approaches like MAT \cite{wen2022multi}, which leverage advanced architectures to improve coordination. These methods have achieved remarkable success in benchmark environments shown in the Section 2.1. 
% 尽管协作多智能体强化学习（MARL）已成为解决协作场景中大规模、复杂、实时和不确定问题的强大框架，但它也带来了一些需要解决的挑战。首先，在部分可观测环境中，智能体只能获取局部信息，这使得它们难以独立做出最优决策[20]。其次，多个智能体同时学习导致环境非平稳，增加了策略收敛的难度[21]。第三，协作式MAS通常依赖于共享奖励，这引发了信用分配问题——如何有效分配奖励以引导个体智能体实现协作并最大化系统性能[22]。最后，随着智能体数量的增加，策略学习的搜索空间呈指数级增长，带来了可扩展性问题，使得策略优化变得愈发困难[23, 24]。
\subsection{Human-AI collaboration}
Much recent work has focused on designing collaborative agents towards humans in the original \textit{Overcooked\_AI} environment. Behavioral cloning play (BCP) \cite{carroll2019utility} is the first proposed algorithm for human-AI collaboration in \textit{Overcooked}, which relies on human experts' data. Fictitious Co-Play (FCP) \cite{strouse2021collaborating} is a zero-shot coordination (ZSC) (also known as ad-hoc team-play \cite{stone2010ad}) algorithm that mainly consists of two stages. In the first stage, it trains a population of self-play agents and save their ``checkpoint'' representing their strategy at that point in time. In the second stage, it trains the FCP agent by teaming up it with various ``checkpoints''. While training FCP agent is somewhat time-consuming and resource-intensive, it performs well when it collaborates with unseen humans. Subsequent works following FCP have augmented the policy pool for a stronger adaptive policy by using biased strategy \cite{yu2023learning} or population entropy bonus \cite{zhao2023maximum}. Some other algorithms also focus on exploring the edge cases of collaborative agents \cite{knott2021evaluating}, modeling systematic suboptimality of humans \cite{laidlaw2022boltzmann} and reusing optimal policy via Bayesian policy reuse \cite{wangbeyond} to improve the human's performance in collaborative tasks. 
% \subsection{Multi-agent task allocation}
% Task allocation has been extensively studied in the field of multi-agent systems\cite{gerkey2004formal,khamis2015multi}. In this context, the primary objective is to effectively assign agents to tasks from a given set, with the aim of maximizing overall utility. It is assumed that a utility function, which associates utility values with agent team-task pairs, is provided for this purpose. This allocation process plays a crucial role in optimizing the overall performance and efficiency of multi-agent systems and robotic operations.
% According to taxonomy of task allocation in multi-agent systems \cite{gerkey2004formal}, \textit{Overcooked pygame} problem can be classified into ST-MR-IA (single-task agents, multi-agent tasks, instantaneous allocation). That is, multiple tasks need to be executed simultaneously, and the same task requires collaboration among multiple agents to complete. Recently, some works focus on allocating agents to different subtasks. However, these subtasks offen have a fixed workflow \cite{iqbal2022alma} or lack semantic meanings as a latent variable \cite{yang2022ldsa}. \textit{Overcooked pygame} is an environment where multiple tasks should be executed synchronously, and the workflow of each subtask varies due to the behavioral preferences of agents, posing a challenge in the field of task allocation.